library(this.path)
library(patchwork)
library(ggExtra)
library(hetGP)
library(pracma)
library(cowplot)
library(parallel)
library(GGally)
library(doParallel)
library(ks)
setwd(this.path::here())
source("helpers.R")
set.seed(13)
# read in some example data
curr_date = 'jul2023_3'
all_dates = c('jul2023_1', 'jul2023_2', 'jul2023_3', 'jul2023_4', 'jun2024_1',
'mar2024_1', 'mar2024_2', 'may2023_1', 'oct2024_1') #, 'oct2024_2'
min_average = '30avg'
drm_file_name = '../../sabrs-rampart/effective_area/eff_area_1d/sabrs3_stpsat6_drm_pHemi_SABRS3only_eventSD_emsop3_hadqgspbert_multi_merged_threshold0.10.csv'
neutron_var = "h1.0.1MeV"
data_path = paste0("../protons_to_neutrons/", curr_date,
"/data_logs/neutron_cnts_and_fit_band_functions_", min_average,"_",
curr_date, ".csv")
neutron_data = as_tibble(read.csv(data_path))
neutron_data$analysis = curr_date
neutron_data$subset = 'current'
neutron_data[[neutron_var]] = log(neutron_data[[neutron_var]])
for(other_date in setdiff(all_dates, curr_date)){
data_path = paste0("../protons_to_neutrons/", other_date,
"/data_logs/neutron_cnts_and_fit_band_functions_", min_average,"_",
other_date, ".csv")
tmp_df = as_tibble(read.csv(data_path))
tmp_df$analysis = other_date
tmp_df$subset = 'past'
tmp_df[[neutron_var]] = log(tmp_df[[neutron_var]])
neutron_data = rbind(neutron_data, tmp_df)
}
## Let's reparameterize so that gamma_b is really the difference between gamma_a and gamma_b
neutron_data$gamma_b = neutron_data$gamma_b - neutron_data$gamma_a
# For now, let's work on h1 with the 0.1 cutoff.
neutron_data = neutron_data %>%
select(Timestamp, A, e0, gamma_a, gamma_b, neutron_var, analysis) %>%
mutate(across(c(A, e0, gamma_a, gamma_b), log))
p_A_base <- ggplot(neutron_data, aes(x = .data[[neutron_var]], y = A, color = analysis)) +
geom_point() +
labs(title = paste0("A vs ", neutron_var)) +
ylab("log(A)") + xlab(paste0('log(', neutron_var, ')')) +
theme_minimal()
p_e0_base <- ggplot(neutron_data, aes(x = .data[[neutron_var]], y = e0, color = analysis)) +
geom_point() +
labs(title = paste0("e0 vs ", neutron_var)) +
ylab("log(e0)") + xlab(paste0('log(', neutron_var, ')')) +
theme_minimal()
p_gammaa_base <- ggplot(neutron_data, aes(x = .data[[neutron_var]], y = gamma_a, color = analysis)) +
geom_point() +
labs(title = paste0("gamma_a vs ", neutron_var)) +
ylab("log(gamma_a)") + xlab(paste0('log(', neutron_var, ')')) +
theme_minimal()
p_gammab_base <- ggplot(neutron_data, aes(x = .data[[neutron_var]], y = gamma_b, color = analysis)) +
geom_point() +
labs(title = paste0("gamma_b vs ", neutron_var)) +
ylab("log(gamma_b - gamma_a)") + xlab(paste0('log(', neutron_var, ')')) +
theme_minimal()
# Add marginal histograms
p_A <- ggMarginal(p_A_base, type = "histogram", margins = "both")
p_e0 <- ggMarginal(p_e0_base, type = "histogram", margins = "both")
p_gammaa <- ggMarginal(p_gammaa_base, type = "histogram", margins = "both")
p_gammab <- ggMarginal(p_gammab_base, type = "histogram", margins = "both")
grid <- (
wrap_elements(p_A) | wrap_elements(p_e0)
) /
(
wrap_elements(p_gammaa) | wrap_elements(p_gammab)
)
grid + plot_annotation(title=paste0("Spread of Parameters Across SEP Events; Current Analysis Date ", curr_date))
# Variables to model
response_vars <- c("A", "e0", "gamma_a", "gamma_b")
# Perform a leave-one-out experiment fitting a Bayesian model for every point.
n_samples = floor(200000)
n_burnin = floor(n_samples*0.333)
proposal_scale <- 0.1
energy_grid <- seq(7.5, 500, length.out = 500)
num_of_bins <- 50
parfctn = function(leave_out_idx){
# for(leave_out_idx in 1:nrow(neutron_data)){
library(ggplot2)
library(tidyverse)
library(this.path)
library(patchwork)
library(ggExtra)
library(hetGP)
library(GGally)
library(parallel)
library(pracma)
library(cowplot)
library(doParallel)
library(ks)
library(cowplot)
source("helpers.R")
indices_not_including_curr_date = which(neutron_data$analysis != curr_date)
subset_neutron_data = neutron_data[indices_not_including_curr_date,]
######
# Here is where I define the prior based on previous data
theta_samples <- subset_neutron_data[, response_vars]
Z_values   <- unlist(subset_neutron_data[, neutron_var])
bin_labels <- cut(Z_values, breaks = quantile(Z_values, probs = seq(0, 1, 1/num_of_bins)),
include.lowest = TRUE)
# Based on the binning above of the Z_values, I can pre-compute all of the slice
# priors to speed up the MH algorithm
kdes_of_theta_subsamples = list()
print(paste0("Pre-computing the piece-wise slice priors..."))
for(new_bin in unique(bin_labels)){
theta_subset <- theta_samples[bin_labels == new_bin, ]
theta_cov <- cov(theta_subset)
H_custom <- theta_cov
kde_fit <- ks::kde(theta_subset, H = H_custom)
kdes_of_theta_subsamples[[new_bin]] = kde_fit
}
# With the precomputed slice priors, my overall prior is essentially a stepwise function
# Together this isn't a density, but the normalization terms will cancel out in the full MH ratio.
log_prior_theta_and_Z <- function(theta, Z_new){
new_bin <- cut(Z_new, breaks = quantile(Z_values, probs = seq(0, 1, 1/num_of_bins)), include.lowest = TRUE)
return(log(predict(kdes_of_theta_subsamples[[new_bin]], x = matrix(theta, nrow = 1))))
}
sample_from_prior = function(Z_new){
# Find the corresponding bin
new_bin <- cut(Z_new,
breaks = quantile(Z_values, probs = seq(0, 1, 1/num_of_bins)),
include.lowest = TRUE)
# Sample theta from the KDE associated with that bin
theta_sampled <- ks::rkde(1, fhat = kdes_of_theta_subsamples[[new_bin]])
return(theta_sampled)
}
# Use the band function and an optimization scheme to see about getting something in the ballpark
# of the correct value.
optim_function = function(x){ (integrate_neutron_counts(drm_file_name, x, neutron_var)-
unlist(neutron_data[leave_out_idx, neutron_var]) )^2 }
random_observed_point = sample(1:nrow(subset_neutron_data), 1)
x0 = unlist(subset_neutron_data[random_observed_point, response_vars])
initial_theta = fmincon(x0, optim_function,
lb = apply(subset_neutron_data[,2:5], 2, min),
ub = apply(subset_neutron_data[,2:5], 2, max))$par
######
# Perform the MH sampling
mh_samples = mh_sampler(Z = unlist(neutron_data[leave_out_idx,neutron_var]),
log_prior_theta_and_Z = log_prior_theta_and_Z,
n_iter = n_samples,
proposal_sds = proposal_scale*diag(cov(theta_samples)),
init_theta = unlist(subset_neutron_data[random_observed_point, response_vars]),
sd_of_likelihood = 0.2*sd(unlist(subset_neutron_data[, neutron_var])),
drm_file_name,
neutron_var)
full_mh_samples = mh_samples$samples
log_posteriors = mh_samples$log_posteriors
log_priors = mh_samples$log_priors
log_likelihoods = mh_samples$log_likelihoods
integrated_fluxes = mh_samples$integrated_fluxes
mh_samples = mh_samples$samples[n_burnin:n_samples,]
AR = nrow(unique(full_mh_samples))/n_samples
#####
# Record traceplot information.
draw_traceplots(neutron_data, full_mh_samples, log_posteriors, log_priors,
log_likelihoods, neutron_var, integrated_fluxes, AR, leave_out_idx,
sample_from_prior,unlist(neutron_data[leave_out_idx,neutron_var]), drm_file_name)
#####
# Look at the sampled values using scatterplots
draw_scatterplots(neutron_data, subset_neutron_data, full_mh_samples,
integrated_fluxes, neutron_var, leave_out_idx)
######
# With the 4-dim samples, get mean estimate, and plot the error bars.  Plot against the truth.
draw_credible_intervals(mh_samples, neutron_data, leave_out_idx, response_vars,
energy_grid, AR, drm_file_name, neutron_var, scale = 2)
}
leave_out_indices = which(neutron_data$analysis == curr_date)
parfctn(2)
# Early development on inverse model for vela project
## Author: AC Murph
## Date: May 2025
library(ggplot2)
library(tidyverse)
library(this.path)
library(patchwork)
library(ggExtra)
library(hetGP)
library(pracma)
library(cowplot)
library(parallel)
library(GGally)
library(doParallel)
library(ks)
setwd(this.path::here())
source("helpers.R")
set.seed(13)
# read in some example data
curr_date = 'jul2023_3'
all_dates = c('jul2023_1', 'jul2023_2', 'jul2023_3', 'jul2023_4', 'jun2024_1',
'mar2024_1', 'mar2024_2', 'may2023_1', 'oct2024_1') #, 'oct2024_2'
min_average = '30avg'
drm_file_name = '../../sabrs-rampart/effective_area/eff_area_1d/sabrs3_stpsat6_drm_pHemi_SABRS3only_eventSD_emsop3_hadqgspbert_multi_merged_threshold0.10.csv'
neutron_var = "h1.0.1MeV"
data_path = paste0("../protons_to_neutrons/", curr_date,
"/data_logs/neutron_cnts_and_fit_band_functions_", min_average,"_",
curr_date, ".csv")
neutron_data = as_tibble(read.csv(data_path))
neutron_data$analysis = curr_date
neutron_data$subset = 'current'
neutron_data[[neutron_var]] = log(neutron_data[[neutron_var]])
for(other_date in setdiff(all_dates, curr_date)){
data_path = paste0("../protons_to_neutrons/", other_date,
"/data_logs/neutron_cnts_and_fit_band_functions_", min_average,"_",
other_date, ".csv")
tmp_df = as_tibble(read.csv(data_path))
tmp_df$analysis = other_date
tmp_df$subset = 'past'
tmp_df[[neutron_var]] = log(tmp_df[[neutron_var]])
neutron_data = rbind(neutron_data, tmp_df)
}
## Let's reparameterize so that gamma_b is really the difference between gamma_a and gamma_b
neutron_data$gamma_b = neutron_data$gamma_b - neutron_data$gamma_a
# For now, let's work on h1 with the 0.1 cutoff.
neutron_data = neutron_data %>%
select(Timestamp, A, e0, gamma_a, gamma_b, neutron_var, analysis) %>%
mutate(across(c(A, e0, gamma_a, gamma_b), log))
p_A_base <- ggplot(neutron_data, aes(x = .data[[neutron_var]], y = A, color = analysis)) +
geom_point() +
labs(title = paste0("A vs ", neutron_var)) +
ylab("log(A)") + xlab(paste0('log(', neutron_var, ')')) +
theme_minimal()
p_e0_base <- ggplot(neutron_data, aes(x = .data[[neutron_var]], y = e0, color = analysis)) +
geom_point() +
labs(title = paste0("e0 vs ", neutron_var)) +
ylab("log(e0)") + xlab(paste0('log(', neutron_var, ')')) +
theme_minimal()
p_gammaa_base <- ggplot(neutron_data, aes(x = .data[[neutron_var]], y = gamma_a, color = analysis)) +
geom_point() +
labs(title = paste0("gamma_a vs ", neutron_var)) +
ylab("log(gamma_a)") + xlab(paste0('log(', neutron_var, ')')) +
theme_minimal()
p_gammab_base <- ggplot(neutron_data, aes(x = .data[[neutron_var]], y = gamma_b, color = analysis)) +
geom_point() +
labs(title = paste0("gamma_b vs ", neutron_var)) +
ylab("log(gamma_b - gamma_a)") + xlab(paste0('log(', neutron_var, ')')) +
theme_minimal()
# Add marginal histograms
p_A <- ggMarginal(p_A_base, type = "histogram", margins = "both")
p_e0 <- ggMarginal(p_e0_base, type = "histogram", margins = "both")
p_gammaa <- ggMarginal(p_gammaa_base, type = "histogram", margins = "both")
p_gammab <- ggMarginal(p_gammab_base, type = "histogram", margins = "both")
grid <- (
wrap_elements(p_A) | wrap_elements(p_e0)
) /
(
wrap_elements(p_gammaa) | wrap_elements(p_gammab)
)
grid + plot_annotation(title=paste0("Spread of Parameters Across SEP Events; Current Analysis Date ", curr_date))
# Variables to model
response_vars <- c("A", "e0", "gamma_a", "gamma_b")
# Perform a leave-one-out experiment fitting a Bayesian model for every point.
n_samples = floor(20000)
n_burnin = floor(n_samples*0.333)
proposal_scale <- 0.1
energy_grid <- seq(7.5, 500, length.out = 500)
num_of_bins <- 50
parfctn = function(leave_out_idx){
# for(leave_out_idx in 1:nrow(neutron_data)){
library(ggplot2)
library(tidyverse)
library(this.path)
library(patchwork)
library(ggExtra)
library(hetGP)
library(GGally)
library(parallel)
library(pracma)
library(cowplot)
library(doParallel)
library(ks)
library(cowplot)
source("helpers.R")
indices_not_including_curr_date = which(neutron_data$analysis != curr_date)
subset_neutron_data = neutron_data[indices_not_including_curr_date,]
######
# Here is where I define the prior based on previous data
theta_samples <- subset_neutron_data[, response_vars]
Z_values   <- unlist(subset_neutron_data[, neutron_var])
bin_labels <- cut(Z_values, breaks = quantile(Z_values, probs = seq(0, 1, 1/num_of_bins)),
include.lowest = TRUE)
# Based on the binning above of the Z_values, I can pre-compute all of the slice
# priors to speed up the MH algorithm
kdes_of_theta_subsamples = list()
print(paste0("Pre-computing the piece-wise slice priors..."))
for(new_bin in unique(bin_labels)){
theta_subset <- theta_samples[bin_labels == new_bin, ]
theta_cov <- cov(theta_subset)
H_custom <- theta_cov
kde_fit <- ks::kde(theta_subset, H = H_custom)
kdes_of_theta_subsamples[[new_bin]] = kde_fit
}
# With the precomputed slice priors, my overall prior is essentially a stepwise function
# Together this isn't a density, but the normalization terms will cancel out in the full MH ratio.
log_prior_theta_and_Z <- function(theta, Z_new){
new_bin <- cut(Z_new, breaks = quantile(Z_values, probs = seq(0, 1, 1/num_of_bins)), include.lowest = TRUE)
return(log(predict(kdes_of_theta_subsamples[[new_bin]], x = matrix(theta, nrow = 1))))
}
sample_from_prior = function(Z_new){
# Find the corresponding bin
new_bin <- cut(Z_new,
breaks = quantile(Z_values, probs = seq(0, 1, 1/num_of_bins)),
include.lowest = TRUE)
# Sample theta from the KDE associated with that bin
theta_sampled <- ks::rkde(1, fhat = kdes_of_theta_subsamples[[new_bin]])
return(theta_sampled)
}
# Use the band function and an optimization scheme to see about getting something in the ballpark
# of the correct value.
optim_function = function(x){ (integrate_neutron_counts(drm_file_name, x, neutron_var)-
unlist(neutron_data[leave_out_idx, neutron_var]) )^2 }
random_observed_point = sample(1:nrow(subset_neutron_data), 1)
x0 = unlist(subset_neutron_data[random_observed_point, response_vars])
initial_theta = fmincon(x0, optim_function,
lb = apply(subset_neutron_data[,2:5], 2, min),
ub = apply(subset_neutron_data[,2:5], 2, max))$par
######
# Perform the MH sampling
mh_samples = mh_sampler(Z = unlist(neutron_data[leave_out_idx,neutron_var]),
log_prior_theta_and_Z = log_prior_theta_and_Z,
n_iter = n_samples,
proposal_sds = proposal_scale*diag(cov(theta_samples)),
init_theta = unlist(subset_neutron_data[random_observed_point, response_vars]),
sd_of_likelihood = 0.2*sd(unlist(subset_neutron_data[, neutron_var])),
drm_file_name,
neutron_var)
full_mh_samples = mh_samples$samples
log_posteriors = mh_samples$log_posteriors
log_priors = mh_samples$log_priors
log_likelihoods = mh_samples$log_likelihoods
integrated_fluxes = mh_samples$integrated_fluxes
mh_samples = mh_samples$samples[n_burnin:n_samples,]
AR = nrow(unique(full_mh_samples))/n_samples
#####
# Record traceplot information.
draw_traceplots(neutron_data, full_mh_samples, log_posteriors, log_priors,
log_likelihoods, neutron_var, integrated_fluxes, AR, leave_out_idx,
sample_from_prior,unlist(neutron_data[leave_out_idx,neutron_var]), drm_file_name)
#####
# Look at the sampled values using scatterplots
draw_scatterplots(neutron_data, subset_neutron_data, full_mh_samples,
integrated_fluxes, neutron_var, leave_out_idx)
######
# With the 4-dim samples, get mean estimate, and plot the error bars.  Plot against the truth.
draw_credible_intervals(mh_samples, neutron_data, leave_out_idx, response_vars,
energy_grid, AR, drm_file_name, neutron_var, scale = 2)
}
leave_out_indices = which(neutron_data$analysis == curr_date)
parfctn(2)
# sockettype <- "PSOCK"
# ncores <- 10
# cl <- parallel::makeCluster(spec = ncores,type = sockettype) #, outfile=""
# setDefaultCluster(cl)
# registerDoParallel(cl)
# sim_ts <- foreach(i=leave_out_indices,
#                   .verbose = T)%dopar%{
#                     print(i)
#                     parfctn(i)
#                   }
# stopCluster(cl)
paste0(this.path::here(), "helpers.R")
# Early development on inverse model for vela project
## Author: AC Murph
## Date: May 2025
library(ggplot2)
library(tidyverse)
library(this.path)
library(patchwork)
library(ggExtra)
library(hetGP)
library(pracma)
library(cowplot)
library(parallel)
library(GGally)
library(doParallel)
library(ks)
# setwd(this.path::here())
source(paste0(this.path::here(), "/helpers.R"))
set.seed(13)
args <- commandArgs(trailingOnly = TRUE)
get_arg <- function(name, args) {
value <- sub(paste0("^", name, "="), "", args[grep(paste0("^", name, "="), args)])
if (length(value) == 0) return(NA)
return(value)
}
curr_date <- get_arg("curr_date", args)
neutron_var <- get_arg("neutron_var", args)
cat("Received curr_date:", curr_date, "\n")
cat("Received neutron_var:", neutron_var, "\n")
if(is.na(curr_date)|is.na(neutron_var)){
curr_date = 'jul2023_3'
neutron_var <- "h1.0.1MeV"
}
?cv.glmnet
############################################################
## Two-Dataset Stacking Simulation:
## - Small "original" dataset (what we care about)
## - Larger "representative" dataset (for learning weights)
## We:
##   1. Learn stacking weights on representative data
##   2. Fit base learners on original data
##   3. Apply equal-weights vs transferred-stacking to
##      original test set and compare MSE
############################################################
set.seed(123)
## Packages ----
## Install if needed:
## install.packages(c("randomForest", "glmnet"))
library(randomForest)
library(glmnet)
library(parallel)
library(doParallel)
?cv.glmnet
?glmnet
# Examine how well the synthetic data represent features in epifforma.
## Author: DA Osthus, LJ Beesley, and AC Murph
## Date: April 2025
######################
### Body of Script ###
######################
library(ggplot2)
library(data.table)
library(GGally)
library(viridis)
library(dplyr)
library(ggrepel)
library(this.path)
library(gridExtra)
library(ggExtra)
library(grid)
library(umap)
library(stats)
library(this.path)
setwd(paste0(this.path::here(), '/../'))
theme_set(theme_classic())
screen_multivariate_outliers <- function(df, threshold = 0.975) {
df_numeric <- df[sapply(df, is.numeric)]
center <- colMeans(df_numeric, na.rm = TRUE)
cov_matrix <- cov(df_numeric, use = "complete.obs")
dists <- mahalanobis(df_numeric, center, cov_matrix)
cutoff <- qchisq(threshold, df = ncol(df_numeric))
return(df_numeric[dists < cutoff,])
}
my_path = paste0(this.path::here(), '/../')
savepath <- paste0(my_path,"evaluate_model/evaluation/")
FILES = list.files(savepath)
outlier_threshold = 0.85
############################
### Read in and Organize ###
############################
DISEASE_KEY = 'Dengue'
### demonstrating using Dengue
output_list = readRDS(paste0(savepath,FILES[grepl(DISEASE_KEY,FILES, ignore.case = T)]))
sim_list = readRDS(paste0(savepath,FILES[grepl('synthetic_training',FILES)]))
### organize features
FEATURE_LIST = setdiff(names(output_list['features']$features),c('geography','disease','last_obs_time','h'))
features_wide = output_list['features']$features
train_data = sim_list['features']$features
rm(output_list)
as_tibble(train_data)
glimpse(train_data)
as_tibble(features_wide)
this.path::here()
output_list['features']$features
# Examine how well the synthetic data represent features in epifforma.
## Author: DA Osthus, LJ Beesley, and AC Murph
## Date: April 2025
######################
### Body of Script ###
######################
library(ggplot2)
library(data.table)
library(GGally)
library(viridis)
library(dplyr)
library(ggrepel)
library(this.path)
library(gridExtra)
library(ggExtra)
library(grid)
library(umap)
library(stats)
library(this.path)
setwd(paste0(this.path::here(), '/../'))
theme_set(theme_classic())
screen_multivariate_outliers <- function(df, threshold = 0.975) {
df_numeric <- df[sapply(df, is.numeric)]
center <- colMeans(df_numeric, na.rm = TRUE)
cov_matrix <- cov(df_numeric, use = "complete.obs")
dists <- mahalanobis(df_numeric, center, cov_matrix)
cutoff <- qchisq(threshold, df = ncol(df_numeric))
return(df_numeric[dists < cutoff,])
}
my_path = paste0(this.path::here(), '/../')
savepath <- paste0(my_path,"evaluate_model/evaluation/")
FILES = list.files(savepath)
outlier_threshold = 0.85
############################
### Read in and Organize ###
############################
DISEASE_KEY = 'Dengue'
### demonstrating using Dengue
output_list = readRDS(paste0(savepath,FILES[grepl(DISEASE_KEY,FILES, ignore.case = T)]))
sim_list = readRDS(paste0(savepath,FILES[grepl('synthetic_training',FILES)]))
output_list['features']$features)
output_list['features']$features
